import numpy as np
import pandas as pd
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                             f1_score, roc_auc_score, confusion_matrix, 
                             classification_report, average_precision_score,
                             log_loss)

def calculate_binary_metrics(y_true, y_pred, y_pred_proba=None, model_name="Model"):
    """
    –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –æ—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –±–∏–Ω–∞—Ä–Ω–æ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏
    
    Parameters:
    y_true : array-like, true labels
    y_pred : array-like, predicted labels
    y_pred_proba : array-like, predicted probabilities (optional)
    model_name : str, name of the model for display
    
    Returns:
    dict: Dictionary with all metrics
    """
    
    # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)
    
    # Confusion matrix components
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0  # True Negative Rate
    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0  # False Positive Rate
    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0  # False Negative Rate
    
    # –ú–µ—Ç—Ä–∏–∫–∏, —Ç—Ä–µ–±—É—é—â–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏
    metrics_with_proba = {}
    if y_pred_proba is not None:
        try:
            roc_auc = roc_auc_score(y_true, y_pred_proba)
            pr_auc = average_precision_score(y_true, y_pred_proba)
            logloss = log_loss(y_true, y_pred_proba)
            
            metrics_with_proba = {
                'ROC-AUC': roc_auc,
                'PR-AUC': pr_auc,
                'Log-Loss': logloss
            }
        except:
            print("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å—Å—á–∏—Ç–∞—Ç—å –º–µ—Ç—Ä–∏–∫–∏, —Ç—Ä–µ–±—É—é—â–∏–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π")
    
    # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º rates
    total = len(y_true)
    positive_rate = np.mean(y_true)
    predicted_positive_rate = np.mean(y_pred)
    
    # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –º–µ—Ç—Ä–∏–∫–∏ –≤ —Å–ª–æ–≤–∞—Ä—å
    metrics = {
        'Accuracy': accuracy,
        'Precision': precision,
        'Recall (Sensitivity)': recall,
        'Specificity': specificity,
        'F1-Score': f1,
        'False Positive Rate': fpr,
        'False Negative Rate': fnr,
        'True Positives': tp,
        'True Negatives': tn,
        'False Positives': fp,
        'False Negatives': fn,
        'Total Samples': total,
        'Positive Rate (Actual)': positive_rate,
        'Positive Rate (Predicted)': predicted_positive_rate
    }
    
    # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Å –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è–º–∏ –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
    metrics.update(metrics_with_proba)
    
    # –ö—Ä–∞—Å–∏–≤–æ –≤—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"üìä –ú–ï–¢–†–ò–ö–ò –ë–ò–ù–ê–†–ù–û–ô –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò - {model_name}")
    print("=" * 60)
    
    # –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    print("\nüéØ –û–°–ù–û–í–ù–´–ï –ú–ï–¢–†–ò–ö–ò:")
    print(f"Accuracy:          {accuracy:.4f}")
    print(f"Precision:         {precision:.4f}")
    print(f"Recall:            {recall:.4f}")
    print(f"F1-Score:          {f1:.4f}")
    print(f"Specificity:       {specificity:.4f}")
    
    if y_pred_proba is not None and 'ROC-AUC' in metrics:
        print(f"ROC-AUC:           {metrics['ROC-AUC']:.4f}")
        print(f"PR-AUC:            {metrics['PR-AUC']:.4f}")
        print(f"Log-Loss:          {metrics['Log-Loss']:.4f}")
    
    # Confusion Matrix
    print(f"\nüìã CONFUSION MATRIX:")
    print(f"                Predicted 0   Predicted 1")
    print(f"Actual 0         {tn:8d}       {fp:8d}")
    print(f"Actual 1         {fn:8d}       {tp:8d}")
    
    # Rates
    print(f"\nüìà –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"True Positives:  {tp}")
    print(f"True Negatives:  {tn}")
    print(f"False Positives: {fp} (Type I Error)")
    print(f"False Negatives: {fn} (Type II Error)")
    print(f"Total Samples:   {total}")
    print(f"Actual Positive Rate:   {positive_rate:.3f}")
    print(f"Predicted Positive Rate: {predicted_positive_rate:.3f}")
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–∞—Å—á–µ—Ç—ã
    print(f"\nüîç –î–û–ü–û–õ–ù–ò–¢–ï–õ–¨–ù–û:")
    print(f"False Positive Rate: {fpr:.4f}")
    print(f"False Negative Rate: {fnr:.4f}")
    
    return metrics

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π
def compare_models_metrics(models_metrics, metric_names=None):
    """
    –°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö –º–æ–¥–µ–ª–µ–π
    """
    if metric_names is None:
        metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']
    
    comparison_data = {}
    for model_name, metrics in models_metrics.items():
        comparison_data[model_name] = {metric: metrics.get(metric, np.nan) for metric in metric_names}
    
    comparison_df = pd.DataFrame(comparison_data).T
    print("üìà –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô:")
    print(comparison_df.round(4))
    
    return comparison_df
